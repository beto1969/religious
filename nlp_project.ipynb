{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-25T17:59:48.409564Z",
     "start_time": "2024-10-25T17:59:48.359845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/humbertoaguilar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/humbertoaguilar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/humbertoaguilar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/humbertoaguilar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "additional_stop_words = {'thou', 'thy', 'thee', 'shall', 'unto', 'ye', 'nephi', 'alma', 'god', 'lord', 'one', 'name', 'ahura',\n",
    "                         'tao','said', 'jacob', 'hunahpú', 'u', 'say', 'mazda', 'יהוה', 'joseph',\n",
    "                         'arjuna', 'balam', 'abraham', 'nanak', 'guru', 'naam', 'mehl', 'pas', 'xibalba',\n",
    "                         'dhammas', 'us', 'tathāgata', 'fravashi', 'august', 'augustness', 'great',\n",
    "                         'holy', 'true', 'things', 'truth', 'pause', 'har', 'made', 'came', 'prince',\n",
    "                         'krishna', 'soul', 'life', 'hath', 'know', 'pass', 'behold', 'upon', 'also',\n",
    "                         'even', 'called', 'lords', 'went', 'boys', 'well', 'come', 'therefore',\n",
    "                         'without', 'blessed', 'vucub', 'two', 'thus', 'like', 'way', 'virtue',\n",
    "                         'heb', 'self', 'yet', 'yea', 'mosiah', 'lamanites', 'dhamma', 'go',\n",
    "                         'tohil', 'xbalanqué', 'zarathushtra', 'saint', 'brethren', 'would', 'ānanda',\n",
    "                         'saying', 'princess', 'heavenly', 'names', 'camé', 'doth', 'quitzé', 'shabad',\n",
    "                         'glory', 'rightly', 'eight', 'hun', 'four', 'verily', 'helaman', 'chapter',\n",
    "                         'see', 'within', 'let', 'may', 'indeed', 'sent', 'tribes', 'father', 'son',\n",
    "                         'sons', 'isaac', 'wife', 'signs', 'deity', 'deities', 'grace', 'obtained',\n",
    "                         'heart', 'king', 'according', 'wise', 'pharoah', 'brother', 'gave', 'next', 'p',\n",
    "                         'esau', 'first', 'yamato_take', 'nephites', 'abram', 'pharaoh'}\n",
    "\n",
    "combined_stop_words = stop_words.union(additional_stop_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T17:59:48.531925Z",
     "start_time": "2024-10-25T17:59:48.434504Z"
    }
   },
   "id": "48c8169f777209fb"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    words = nltk.word_tokenize(text.lower())  # Tokenize the text\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in combined_stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def generate_ngrams(texts, n=3):\n",
    "    bigram = Phrases(texts, min_count=3, threshold=10)\n",
    "    trigram = Phrases(bigram[texts], threshold=10)\n",
    "    texts = [trigram[bigram[text]] for text in texts]\n",
    "\n",
    "    # Ensure filtering of n-grams\n",
    "    filtered_texts = [[word for word in text if word not in combined_stop_words and '_' not in word] for text in texts]\n",
    "\n",
    "    return filtered_texts\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T17:59:48.532746Z",
     "start_time": "2024-10-25T17:59:48.435753Z"
    }
   },
   "id": "6b79f13fd3c77867"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv('Dataset_with_Text.csv')\n",
    "\n",
    "documents = df['Text'].tolist()\n",
    "names = df['Name_of_Text'].tolist()\n",
    "\n",
    "# Process each text separately\n",
    "topics_by_document = {}\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "    processed_doc = preprocess(document)\n",
    "    processed_doc = generate_ngrams([processed_doc])[0]\n",
    "\n",
    "    dictionary = corpora.Dictionary([processed_doc])\n",
    "    corpus = [dictionary.doc2bow(processed_doc)]\n",
    "\n",
    "    # Train LDA model for the specific document\n",
    "    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)  # Adjust num_topics as needed\n",
    "\n",
    "    # Extract the top words for each topic\n",
    "    topics = []\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        topic_words = [word.split('*')[1].strip('\"') for word in topic.split(' + ')]\n",
    "        # Remove duplicates, filter out stop words, and ensure exactly 10 words\n",
    "        topic_words = [word for word in dict.fromkeys(topic_words) if word not in combined_stop_words][:10]\n",
    "        while len(topic_words) < 10:\n",
    "            topic_words.append(\"\")  # Add empty strings to maintain length\n",
    "        topics.append(', '.join(topic_words))\n",
    "\n",
    "    topics_by_document[names[i]] = topics\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T18:00:00.322232Z",
     "start_time": "2024-10-25T17:59:48.454853Z"
    }
   },
   "id": "88cb2318c739c56d"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhagavad Gita: work, mind, men, world, man, love, passion, sense, act, birth, work, world, worship, faith, mind, men, act, sense, good, brahma, work, world, mind, man, men, faith, act, sense, worship, spirit, work, world, mind, peace, faith, man, men, sense, spirit, act, work, man, world, faith, mind, end, light, sense, sacrifice, set\n",
      "Book of Mormon: people, many, word, land, ether, day, wherefore, might, man, moroni, people, land, many, word, day, moroni, wherefore, began, might, ether, people, word, land, many, might, wherefore, moroni, ether, began, day, people, land, word, many, might, ether, began, day, forth, among, people, many, word, land, wherefore, forth, began, moroni, power, man\n",
      "Digha Nikaya: monk, perception, body, feeling, form, friend, three, mind, world, discerns, monk, body, perception, form, world, three, friend, mind, feeling, seven, monk, body, friend, form, three, perception, world, mind, feeling, regard, monk, form, perception, feeling, three, body, world, mind, consciousness, devas, monk, world, perception, body, form, three, friend, seven, mind, regard\n",
      "Torah Genesis: land, daughter, earth, day, servant, took, house, man, water, lit, land, earth, servant, daughter, day, house, lit, took, water, egypt, earth, daughter, land, lit, day, man, child, servant, water, saw, land, daughter, man, day, took, earth, place, brought, water, lit, land, daughter, day, took, earth, brought, house, lit, servant, time\n",
      "Kojiki: land, born, sword, child, island, possessor, heaven, elder, thine, mountain, born, land, child, island, sword, mountain, thine, heaven, elder, male, land, born, child, island, birth, heaven, sword, possessor, river, mountain, land, born, child, sword, island, possessor, heaven, elder, mountain, river, born, land, child, sword, elder, island, mountain, possessor, thereupon, hand\n",
      "Popol Vuh: house, people, men, grandmother, tell, answered, face, could, town, began, people, house, men, face, tell, began, grandmother, many, answered, could, house, people, men, man, immediately, began, tell, face, place, answered, house, people, men, grandmother, road, give, order, town, place, must, house, people, grandmother, answered, men, town, tell, place, face, tree\n",
      "Quran: believe, among, people, day, men, apostle, evil, make, give, fear, people, day, believe, men, apostle, among, fear, earth, good, give, believe, people, day, among, apostle, fear, men, good, man, give, people, day, among, men, man, earth, believe, apostle, fear, give, day, people, men, apostle, among, believe, good, fear, man, unbeliever\n",
      "Guru Granth Sahib: mind, world, body, love, word, master, chant, perfect, meditate, alone, mind, world, master, word, body, meditate, love, alone, beloved, perfect, mind, world, word, love, master, body, meditate, perfect, beloved, chant, mind, master, love, body, word, beloved, world, alone, meditate, forever, mind, world, master, love, body, word, alone, servant, chant, perfect\n",
      "Tao Te Ching: people, nature, world, hence, act, desire, nation, oneness, take, person, people, nature, hence, act, world, desire, nation, oneness, essence, person, nature, people, hence, world, desire, act, nation, take, person, although, people, nature, hence, world, act, desire, nation, oneness, man, essence, nature, people, world, hence, act, desire, essence, nation, take, man\n",
      "Zend Avesta: worship, sacrifice, man, good, water, fire, earth, world, word, law, worship, good, man, sacrifice, water, fire, word, world, earth, men, worship, man, good, sacrifice, water, word, world, fire, earth, law, worship, good, sacrifice, man, fire, word, water, world, law, earth, worship, good, man, word, world, water, sacrifice, earth, fire, answered\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "for text, topics in topics_by_document.items():\n",
    "    print(f\"{text}: {', '.join(topics)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T18:00:00.324402Z",
     "start_time": "2024-10-25T18:00:00.321376Z"
    }
   },
   "id": "1bd985427e98bfb6"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ReligiousTextAnalyzer:\n",
    "    def __init__(self, texts_df):\n",
    "        \"\"\"\n",
    "        Initialize with a DataFrame containing religious texts and metadata\n",
    "        Expected columns: Name_of_Text, Text, Region, Climate_Data, Year_Approximate\n",
    "        \"\"\"\n",
    "        self.texts_df = texts_df\n",
    "        self.topic_similarities = None\n",
    "        self.environmental_correlations = None\n",
    "        \n",
    "    def calculate_topic_similarities(self, topics_by_document):\n",
    "        \"\"\"\n",
    "        Calculate similarity matrix between documents based on their topics\n",
    "        \"\"\"\n",
    "        texts = list(topics_by_document.keys())\n",
    "        n_texts = len(texts)\n",
    "        similarity_matrix = np.zeros((n_texts, n_texts))\n",
    "        \n",
    "        # Create a flat list of all topics for each text\n",
    "        text_topics = {text: ' '.join(topics) for text, topics in topics_by_document.items()}\n",
    "        \n",
    "        # Calculate TF-IDF vectors for topics\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(text_topics.values())\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        for i in range(n_texts):\n",
    "            for j in range(n_texts):\n",
    "                if i != j:\n",
    "                    similarity = 1 - cosine(\n",
    "                        tfidf_matrix[i].toarray().flatten(),\n",
    "                        tfidf_matrix[j].toarray().flatten()\n",
    "                    )\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "        \n",
    "        self.topic_similarities = pd.DataFrame(\n",
    "            similarity_matrix,\n",
    "            index=texts,\n",
    "            columns=texts\n",
    "        )\n",
    "        return self.topic_similarities\n",
    "    \n",
    "    def analyze_environmental_correlations(self, climate_data):\n",
    "        \"\"\"\n",
    "        Analyze correlations between topic presence and environmental variables\n",
    "        \n",
    "        climate_data: DataFrame with columns for different environmental variables\n",
    "        \"\"\"\n",
    "        # Normalize climate data\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_climate = pd.DataFrame(\n",
    "            scaler.fit_transform(climate_data),\n",
    "            columns=climate_data.columns,\n",
    "            index=climate_data.index\n",
    "        )\n",
    "        \n",
    "        # Calculate correlations between topic similarities and environmental variables\n",
    "        correlations = {}\n",
    "        for var in normalized_climate.columns:\n",
    "            env_similarities = pdist(normalized_climate[var].values.reshape(-1, 1))\n",
    "            topic_similarities_flat = squareform(self.topic_similarities)\n",
    "            correlation = pearsonr(env_similarities, topic_similarities_flat)[0]\n",
    "            correlations[var] = correlation\n",
    "            \n",
    "        self.environmental_correlations = pd.Series(correlations)\n",
    "        return self.environmental_correlations\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"\n",
    "        Create visualizations of the analysis results\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Topic similarity heatmap\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.heatmap(\n",
    "            self.topic_similarities,\n",
    "            cmap='YlOrRd',\n",
    "            annot=True,\n",
    "            fmt='.2f'\n",
    "        )\n",
    "        plt.title('Topic Similarities Between Religious Texts')\n",
    "        \n",
    "        # Environmental correlations\n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.environmental_correlations.plot(kind='bar')\n",
    "        plt.title('Environmental Variable Correlations')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"\n",
    "        Generate a summary report of findings\n",
    "        \"\"\"\n",
    "        report = []\n",
    "        \n",
    "        # Most similar text pairs\n",
    "        similarities = self.topic_similarities.unstack()\n",
    "        top_similarities = similarities[similarities != 1.0].nlargest(5)\n",
    "        report.append(\"Most similar text pairs:\")\n",
    "        for (text1, text2), similarity in top_similarities.items():\n",
    "            report.append(f\"{text1} - {text2}: {similarity:.3f}\")\n",
    "            \n",
    "        # Strongest environmental correlations\n",
    "        report.append(\"\\nStrongest environmental correlations:\")\n",
    "        for var, corr in self.environmental_correlations.nlargest(3).items():\n",
    "            report.append(f\"{var}: {corr:.3f}\")\n",
    "            \n",
    "        return \"\\n\".join(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T18:00:51.314048Z",
     "start_time": "2024-10-25T18:00:44.534923Z"
    }
   },
   "id": "e2e614dd1520bd57"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/k5/8h71m_tx5pg6yssd4ssc1mbc0000gn/T/ipykernel_53692/968357052.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;34m'water_proximity'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m }, index=df['Name_of_Text'])\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;31m# Analyze correlations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0mcorrelations\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0manalyzer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0manalyze_environmental_correlations\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclimate_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;31m# Visualize results\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0manalyzer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvisualize_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/k5/8h71m_tx5pg6yssd4ssc1mbc0000gn/T/ipykernel_53692/1253984066.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, climate_data)\u001B[0m\n\u001B[1;32m     57\u001B[0m         \"\"\"\n\u001B[1;32m     58\u001B[0m         \u001B[0;31m# Normalize climate data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m         \u001B[0mscaler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMinMaxScaler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         normalized_climate = pd.DataFrame(\n\u001B[0;32m---> 61\u001B[0;31m             \u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclimate_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m             \u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mclimate_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m             \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mclimate_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         )\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/utils/_set_output.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    314\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    315\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 316\u001B[0;31m         \u001B[0mdata_to_wrap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    317\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_to_wrap\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    318\u001B[0m             \u001B[0;31m# only wrap the first output for cross decomposition\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    319\u001B[0m             return_tuple = (\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m   1094\u001B[0m                 )\n\u001B[1;32m   1095\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1096\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1097\u001B[0m             \u001B[0;31m# fit method of arity 1 (unsupervised transformation)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1098\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1099\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1100\u001B[0m             \u001B[0;31m# fit method of arity 2 (supervised transformation)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1101\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    446\u001B[0m             \u001B[0mFitted\u001B[0m \u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    447\u001B[0m         \"\"\"\n\u001B[1;32m    448\u001B[0m         \u001B[0;31m# Reset internal state before fitting\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    449\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 450\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartial_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1469\u001B[0m                 skip_parameter_validation=(\n\u001B[1;32m   1470\u001B[0m                     \u001B[0mprefer_skip_nested_validation\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mglobal_skip_validation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1471\u001B[0m                 )\n\u001B[1;32m   1472\u001B[0m             ):\n\u001B[0;32m-> 1473\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfit_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mestimator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    486\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    487\u001B[0m         \u001B[0mxp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_namespace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    488\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    489\u001B[0m         \u001B[0mfirst_pass\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"n_samples_seen_\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m         X = self._validate_data(\n\u001B[0m\u001B[1;32m    491\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m             \u001B[0mreset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfirst_pass\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    493\u001B[0m             \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_array_api\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msupported_float_dtypes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[1;32m    629\u001B[0m                 \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    630\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    631\u001B[0m                 \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    632\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mno_val_y\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 633\u001B[0;31m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"X\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    634\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mno_val_y\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    635\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m   1009\u001B[0m                         )\n\u001B[1;32m   1010\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mxp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1011\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1012\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_asarray_with_order\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mxp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mxp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1013\u001B[0;31m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1014\u001B[0m                 raise ValueError(\n\u001B[1;32m   1015\u001B[0m                     \u001B[0;34m\"Complex data not supported\\n{}\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1016\u001B[0m                 ) from complex_warning\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/sklearn/utils/_array_api.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(array, dtype, order, copy, xp, device)\u001B[0m\n\u001B[1;32m    741\u001B[0m         \u001B[0;31m# Use NumPy API to support order\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    742\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcopy\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    743\u001B[0m             \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    744\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 745\u001B[0;31m             \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    746\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    747\u001B[0m         \u001B[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    748\u001B[0m         \u001B[0;31m# container that is consistent with the input's namespace.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, dtype, copy)\u001B[0m\n\u001B[1;32m   2149\u001B[0m     def __array__(\n\u001B[1;32m   2150\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnpt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDTypeLike\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mbool_t\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2151\u001B[0m     ) -> np.ndarray:\n\u001B[1;32m   2152\u001B[0m         \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2153\u001B[0;31m         \u001B[0marr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2154\u001B[0m         if (\n\u001B[1;32m   2155\u001B[0m             \u001B[0mastype_is_view\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2156\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0musing_copy_on_write\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "# Create analyzer instance\n",
    "analyzer = ReligiousTextAnalyzer(df)\n",
    "\n",
    "# Calculate similarities using your existing topic analysis\n",
    "similarities = analyzer.calculate_topic_similarities(topics_by_document)\n",
    "\n",
    "# Add environmental data\n",
    "climate_data = pd.DataFrame({\n",
    "    'annual_rainfall': [...],\n",
    "    'avg_temperature': [...],\n",
    "    'elevation': [...],\n",
    "    'water_proximity': [...]\n",
    "}, index=df['Name_of_Text'])\n",
    "\n",
    "# Analyze correlations\n",
    "correlations = analyzer.analyze_environmental_correlations(climate_data)\n",
    "\n",
    "# Visualize results\n",
    "analyzer.visualize_results()\n",
    "\n",
    "# Generate report\n",
    "print(analyzer.generate_report())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T18:00:51.429731Z",
     "start_time": "2024-10-25T18:00:51.313807Z"
    }
   },
   "id": "9e6a04f9fcb526c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3b2d913bcb5457ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
